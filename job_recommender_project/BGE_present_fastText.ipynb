{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText\n",
    "========\n",
    "- Made by facebook <https://github.com/facebookresearch/fastText>\n",
    "- Treats each word as the aggregation of its subwords. \n",
    "    - Subwords are character n-grams of the word. (e.g. army --> a, r, m, y, ar, rm, my, arm, rmy,\n",
    "- Pros:\n",
    "    - Much better thahn Word2Vec on syntactic tasks, especially with small training corpus\n",
    "    - fastText can be used to obtain vectors for out-of-vocabulary (OOV) words\n",
    "- Cons\n",
    "    - Slightly worse than Word2Vec semantic tasks\n",
    "    - Slower training time than Word2Vec\n",
    "    - Comparision: <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Credit:\n",
    "- https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-download-auto-examples-tutorials-run-fasttext-py\n",
    "- wm distances work:\n",
    "    - Ofir Pele and Michael Werman “A linear time histogram metric for improved SIFT matching”\n",
    "    - Ofir Pele and Michael Werman “Fast and robust earth mover’s distances”\n",
    "    - Matt Kusner et al. “From Word Embeddings To Document Distances”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "import operator\n",
    "\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Resume Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>dirty_resume</th>\n",
       "      <th>resume</th>\n",
       "      <th>tokenized_resume</th>\n",
       "      <th>lemmatized_resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HR</td>\n",
       "      <td>b'John H. Smith, P.H.R.\\n800-991-5187 | PO Box...</td>\n",
       "      <td>john h smith phr    po box  callahan fl  infog...</td>\n",
       "      <td>[john, h, smith, phr, po, box, callahan, fl, i...</td>\n",
       "      <td>[john, h, smith, phr, po, box, callahan, fl, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>b'Name Surname\\nAddress\\nMobile No/Email\\nPERS...</td>\n",
       "      <td>name surname address mobile noemail personal p...</td>\n",
       "      <td>[name, surname, address, mobile, noemail, pers...</td>\n",
       "      <td>[name, surname, address, mobile, noemail, pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HR</td>\n",
       "      <td>b'Anthony Brown\\nHR Assistant\\nAREAS OF EXPERT...</td>\n",
       "      <td>anthony brown hr assistant areas expertise per...</td>\n",
       "      <td>[anthony, brown, hr, assistant, areas, experti...</td>\n",
       "      <td>[anthony, brown, hr, assistant, area, expertis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>HR</td>\n",
       "      <td>b'www.downloadmela.com\\nSatheesh\\nEMAIL ID:\\nC...</td>\n",
       "      <td>satheesh email id career objective pursue gro...</td>\n",
       "      <td>[satheesh, email, id, career, objective, pursu...</td>\n",
       "      <td>[satheesh, email, id, career, objective, pursu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>HR</td>\n",
       "      <td>b\"HUMAN RESOURCES DIRECTOR\\n\\xef\\x82\\xb7Expert...</td>\n",
       "      <td>human resources director expert organizational...</td>\n",
       "      <td>[human, resources, director, expert, organizat...</td>\n",
       "      <td>[human, resource, director, expert, organizati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Category                                       dirty_resume  \\\n",
       "0   1       HR  b'John H. Smith, P.H.R.\\n800-991-5187 | PO Box...   \n",
       "1   2       HR  b'Name Surname\\nAddress\\nMobile No/Email\\nPERS...   \n",
       "2   3       HR  b'Anthony Brown\\nHR Assistant\\nAREAS OF EXPERT...   \n",
       "3   4       HR  b'www.downloadmela.com\\nSatheesh\\nEMAIL ID:\\nC...   \n",
       "4   5       HR  b\"HUMAN RESOURCES DIRECTOR\\n\\xef\\x82\\xb7Expert...   \n",
       "\n",
       "                                              resume  \\\n",
       "0  john h smith phr    po box  callahan fl  infog...   \n",
       "1  name surname address mobile noemail personal p...   \n",
       "2  anthony brown hr assistant areas expertise per...   \n",
       "3   satheesh email id career objective pursue gro...   \n",
       "4  human resources director expert organizational...   \n",
       "\n",
       "                                    tokenized_resume  \\\n",
       "0  [john, h, smith, phr, po, box, callahan, fl, i...   \n",
       "1  [name, surname, address, mobile, noemail, pers...   \n",
       "2  [anthony, brown, hr, assistant, areas, experti...   \n",
       "3  [satheesh, email, id, career, objective, pursu...   \n",
       "4  [human, resources, director, expert, organizat...   \n",
       "\n",
       "                                   lemmatized_resume  \n",
       "0  [john, h, smith, phr, po, box, callahan, fl, i...  \n",
       "1  [name, surname, address, mobile, noemail, pers...  \n",
       "2  [anthony, brown, hr, assistant, area, expertis...  \n",
       "3  [satheesh, email, id, career, objective, pursu...  \n",
       "4  [human, resource, director, expert, organizati...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_resume_cleaned_lemmatized_tokenized_path = '/Users/richardkuzma/coding/NLP_projects/job_recommender_project/data/resumes_tokenized_lemmatized.pickle'\n",
    "resumes = pd.read_pickle(local_resume_cleaned_lemmatized_tokenized_path)\n",
    "resumes_sentences = resumes['lemmatized_resume'].tolist()\n",
    "resumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'h',\n",
       " 'smith',\n",
       " 'phr',\n",
       " 'po',\n",
       " 'box',\n",
       " 'callahan',\n",
       " 'fl',\n",
       " 'infogreatresumesfastcom',\n",
       " 'approachable',\n",
       " 'innovator',\n",
       " 'passion',\n",
       " 'human',\n",
       " 'resource',\n",
       " 'senior']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list where each element is a list of strings\n",
    "resumes_sentences[0][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training base FT model for resumes\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=11073, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "base_res_model = FT_gensim(\n",
    "    sentences=None,\n",
    "    sg=0, #default to CBOW. if sg=1 then skip-gram\n",
    "    hs=0, #default, if hs=0 & negative =/= 0 then neg. sampling. if hs=1, hierarchical softmax\n",
    "    negative=5, #5 words selected for negative sampling\n",
    "    size=100, #size of vector\n",
    "    alpha=0.025,\n",
    "    min_count=5, # ignore words with fewer than 20 apearances\n",
    "    iter=5,\n",
    "    seed=42,\n",
    "    cbow_mean=1, #uses mean for CBOW. If it =0 then sums CBOW (provided CBOW not SG)\n",
    "    min_n=3, # min length of char n-grams\n",
    "    max_n=6, # max length of char n-grams. If 0 or less than min_n, this turns into W2V\n",
    "    trim_rule=None, #if you had a rule to trim down vocabulary\n",
    "    workers=3 # default\n",
    ")    \n",
    "\n",
    "    \n",
    "# build the vocabulary\n",
    "base_res_model.build_vocab(sentences = resumes_sentences)\n",
    "\n",
    "# train the model\n",
    "base_res_model.train(\n",
    "    sentences=resumes_sentences,\n",
    "    epochs=base_res_model.epochs,\n",
    "    total_examples=base_res_model.corpus_count,\n",
    "    total_words=base_res_model.corpus_total_words\n",
    ")\n",
    "\n",
    "print(base_res_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Need to be able to pick individual jobs to compare to all resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company</th>\n",
       "      <th>JobDescription</th>\n",
       "      <th>RequiredQual</th>\n",
       "      <th>JobRequirement</th>\n",
       "      <th>label</th>\n",
       "      <th>combined</th>\n",
       "      <th>dirty_combined</th>\n",
       "      <th>tokenized_combined</th>\n",
       "      <th>lemmatized_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chief Financial Officer</td>\n",
       "      <td>AMERIA Investment Consulting Company</td>\n",
       "      <td>AMERIA Investment Consulting Company is seekin...</td>\n",
       "      <td>To perform this job successfully, an\\r\\nindivi...</td>\n",
       "      <td>- Supervises financial management and administ...</td>\n",
       "      <td>1</td>\n",
       "      <td>chief financial officer ameria investment cons...</td>\n",
       "      <td>Chief Financial Officer AMERIA Investment Cons...</td>\n",
       "      <td>[chief, financial, officer, ameria, investment...</td>\n",
       "      <td>[chief, financial, officer, ameria, investment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Country Coordinator</td>\n",
       "      <td>Caucasus Environmental NGO Network (CENN)</td>\n",
       "      <td>Public outreach and strengthening of a growing...</td>\n",
       "      <td>- Degree in environmentally related field, or ...</td>\n",
       "      <td>- Working with the Country Director to provide...</td>\n",
       "      <td>2</td>\n",
       "      <td>country coordinator public outreach strengthen...</td>\n",
       "      <td>Country Coordinator Public outreach and streng...</td>\n",
       "      <td>[country, coordinator, public, outreach, stren...</td>\n",
       "      <td>[country, coordinator, public, outreach, stren...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BCC Specialist</td>\n",
       "      <td>Manoff Group</td>\n",
       "      <td>The LEAD (Local Enhancement and Development fo...</td>\n",
       "      <td>- Advanced degree in public health, social sci...</td>\n",
       "      <td>- Identify gaps in knowledge and overseeing in...</td>\n",
       "      <td>3</td>\n",
       "      <td>bcc specialist lead local enhancement developm...</td>\n",
       "      <td>BCC Specialist The LEAD (Local Enhancement and...</td>\n",
       "      <td>[bcc, specialist, lead, local, enhancement, de...</td>\n",
       "      <td>[bcc, specialist, lead, local, enhancement, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Community Development, Capacity Building and C...</td>\n",
       "      <td>Food Security Regional Cooperation and Stabili...</td>\n",
       "      <td>Food Security Regional Cooperation and Stabili...</td>\n",
       "      <td>- Higher Education and/or professional experie...</td>\n",
       "      <td>- Assist the Tavush Marz communities and commu...</td>\n",
       "      <td>4</td>\n",
       "      <td>community development capacity building confli...</td>\n",
       "      <td>Community Development, Capacity Building and C...</td>\n",
       "      <td>[community, development, capacity, building, c...</td>\n",
       "      <td>[community, development, capacity, building, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Country Economist (NOB)</td>\n",
       "      <td>United Nations Development Programme, Armenia</td>\n",
       "      <td>The United Nations Development Programme in Ar...</td>\n",
       "      <td>- Minimum Masters Degree in Economics;\\r\\n- Mi...</td>\n",
       "      <td>The incumbent under direct supervision of UNDP...</td>\n",
       "      <td>5</td>\n",
       "      <td>country economist nob united nations developme...</td>\n",
       "      <td>Country Economist (NOB) The United Nations Dev...</td>\n",
       "      <td>[country, economist, nob, united, nations, dev...</td>\n",
       "      <td>[country, economist, nob, united, nation, deve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                            Chief Financial Officer   \n",
       "1                                Country Coordinator   \n",
       "2                                     BCC Specialist   \n",
       "3  Community Development, Capacity Building and C...   \n",
       "4                            Country Economist (NOB)   \n",
       "\n",
       "                                             Company  \\\n",
       "0               AMERIA Investment Consulting Company   \n",
       "1          Caucasus Environmental NGO Network (CENN)   \n",
       "2                                       Manoff Group   \n",
       "3  Food Security Regional Cooperation and Stabili...   \n",
       "4      United Nations Development Programme, Armenia   \n",
       "\n",
       "                                      JobDescription  \\\n",
       "0  AMERIA Investment Consulting Company is seekin...   \n",
       "1  Public outreach and strengthening of a growing...   \n",
       "2  The LEAD (Local Enhancement and Development fo...   \n",
       "3  Food Security Regional Cooperation and Stabili...   \n",
       "4  The United Nations Development Programme in Ar...   \n",
       "\n",
       "                                        RequiredQual  \\\n",
       "0  To perform this job successfully, an\\r\\nindivi...   \n",
       "1  - Degree in environmentally related field, or ...   \n",
       "2  - Advanced degree in public health, social sci...   \n",
       "3  - Higher Education and/or professional experie...   \n",
       "4  - Minimum Masters Degree in Economics;\\r\\n- Mi...   \n",
       "\n",
       "                                      JobRequirement  label  \\\n",
       "0  - Supervises financial management and administ...      1   \n",
       "1  - Working with the Country Director to provide...      2   \n",
       "2  - Identify gaps in knowledge and overseeing in...      3   \n",
       "3  - Assist the Tavush Marz communities and commu...      4   \n",
       "4  The incumbent under direct supervision of UNDP...      5   \n",
       "\n",
       "                                            combined  \\\n",
       "0  chief financial officer ameria investment cons...   \n",
       "1  country coordinator public outreach strengthen...   \n",
       "2  bcc specialist lead local enhancement developm...   \n",
       "3  community development capacity building confli...   \n",
       "4  country economist nob united nations developme...   \n",
       "\n",
       "                                      dirty_combined  \\\n",
       "0  Chief Financial Officer AMERIA Investment Cons...   \n",
       "1  Country Coordinator Public outreach and streng...   \n",
       "2  BCC Specialist The LEAD (Local Enhancement and...   \n",
       "3  Community Development, Capacity Building and C...   \n",
       "4  Country Economist (NOB) The United Nations Dev...   \n",
       "\n",
       "                                  tokenized_combined  \\\n",
       "0  [chief, financial, officer, ameria, investment...   \n",
       "1  [country, coordinator, public, outreach, stren...   \n",
       "2  [bcc, specialist, lead, local, enhancement, de...   \n",
       "3  [community, development, capacity, building, c...   \n",
       "4  [country, economist, nob, united, nations, dev...   \n",
       "\n",
       "                                 lemmatized_combined  \n",
       "0  [chief, financial, officer, ameria, investment...  \n",
       "1  [country, coordinator, public, outreach, stren...  \n",
       "2  [bcc, specialist, lead, local, enhancement, de...  \n",
       "3  [community, development, capacity, building, c...  \n",
       "4  [country, economist, nob, united, nation, deve...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load jobs into df\n",
    "\n",
    "local_jobs_cleaned_lemmatized_tokenized_path = '/Users/richardkuzma/coding/NLP_projects/job_recommender_project/data/large_files/jobs_tokenized_lemmatized.pickle'\n",
    "jobs = pd.read_pickle(local_jobs_cleaned_lemmatized_tokenized_path)\n",
    "jobs_list = jobs['lemmatized_combined'].tolist()\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13124"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#13124 total jobs\n",
    "jobs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_job(selection=-999):\n",
    "    print(\"There are {} jobs\".format(jobs.shape[0]))\n",
    "    \n",
    "    # Select a random int from 0 to length of rjob set\n",
    "    rand_int = np.random.randint(1, jobs.shape[0]+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if selection == -999:\n",
    "        selection = rand_int\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print ('\\nselected job is ID #{}'.format(selection))\n",
    "    \n",
    "    # pick the job text and ID associated with the random int\n",
    "    job_label = jobs.iloc[selection - 1, jobs.columns.get_loc('label')] #we could grab ID, but this works for non-indexed labels too\n",
    "    job_title = jobs.iloc[selection - 1 ]['Title']\n",
    "    job_company = jobs.iloc[selection - 1 ]['Company']\n",
    "    job_description = jobs.iloc[selection - 1 ]['JobDescription']\n",
    "    \n",
    "    \n",
    "    print('Job Posting ID is: {}'.format(job_label))\n",
    "    print('Job Posting Title: {}'.format(job_title))\n",
    "    print('Job Posting Company: {}'.format(job_company))\n",
    "    print('Job Posting Description: {}'.format(job_description))\n",
    "    \n",
    "    #Convert the sample document into a list and use the infer_vector method to get a vector representation for it\n",
    "    job_text_to_process = jobs['lemmatized_combined'][selection - 1]\n",
    "    \n",
    "    return job_text_to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_job_find_similar_resumes(job_you_pick, model=base_res_model):\n",
    "\n",
    "        \n",
    "    #find all distances between chosen job and each resume\n",
    "    temp_distance = []\n",
    "    min_dist = float(\"inf\")\n",
    "    min_index = float(\"inf\")\n",
    "\n",
    "\n",
    "    # for i in range len(resume_sentences): \n",
    "    for i in range (0, len(resumes_sentences)):\n",
    "        print(i)\n",
    "        dist = model.wmdistance(job_you_pick, resumes_sentences[i])\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_index = i\n",
    "        temp_distance.append((dist, i))\n",
    "\n",
    "    #sort list of tuples\n",
    "    temp_distance.sort(key = operator.itemgetter(0))\n",
    "    temp_distance\n",
    "\n",
    "    num_similar = 10 #or 10, 20, 25\n",
    "    print('\\nPrinting {} most similar candidates...\\n'.format(num_similar))\n",
    "    for i in range(0,num_similar):\n",
    "        print('\\n#{} most similar job'.format(i+1))\n",
    "        print('Resume ID from list: {}'.format(temp_distance[i][1]))\n",
    "        print('WM Distance: {}'.format(temp_distance[i][0]))\n",
    "        print('Resume ID from df: {}'.format(resumes.iloc[temp_distance[i][1]]['ID']))\n",
    "        print('Resume text (500 chars): {}'.format(resumes.iloc[temp_distance[i][1]]['resume'][0:500]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13124 jobs\n",
      "\n",
      "selected job is ID #9670\n",
      "Job Posting ID is: 9670\n",
      "Job Posting Title: Sales Manager\n",
      "Job Posting Company: Navavan LLC\n",
      "Job Posting Description: Navavan LLC is looking for a Sales Manager who will be\r\n",
      "responsible for the development and performance of all sales activities\r\n",
      "in the assigned market. He/ she will staff and direct a sales team and\r\n",
      "provide leadership towards the achievement of maximum profitability and\r\n",
      "growth in line with company vision and values. The incumbent will be also\r\n",
      "responsible for establishing plans and strategies to expand the customer\r\n",
      "base in the marketing area.\n"
     ]
    }
   ],
   "source": [
    "chosen_job = pick_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardkuzma/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `wmdistance` (Method will be removed in 4.0.0, use self.wv.wmdistance() instead).\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-116913e753ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgiven_job_find_similar_resumes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchosen_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-c31696a7b202>\u001b[0m in \u001b[0;36mgiven_job_find_similar_resumes\u001b[0;34m(job_you_pick, model)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresumes_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_you_pick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresumes_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_dist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mmin_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m                 )\n\u001b[0;32m-> 1447\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mwmdistance\u001b[0;34m(self, document1, document2)\u001b[0m\n\u001b[1;32m   1404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1405\u001b[0m         \"\"\"\n\u001b[0;32m-> 1406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.most_similar_cosmul() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mwmdistance\u001b[0;34m(self, document1, document2)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;31m# Compute WMD.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0memd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar_cosmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "given_job_find_similar_resumes(chosen_job)\n",
    "# painfully slow... need GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar(positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training models\n",
    "---------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we'll use the Lee Corpus (which you already have if you've installed gensim) for training our model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_sentences = jobs['lemmatized_combined'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0c96c545b886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_jobs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_jobs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_jobs_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/fasttext.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    923\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madjust_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[1;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n\u001b[0m\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch_corpusfile(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[1;32m    487\u001b[0m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[1;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             report_delay=report_delay, is_corpus_file_mode=False)\n\u001b[0m\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrained_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_word_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_tally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_jobs_model = FT_gensim(\n",
    "    sentences=None,\n",
    "    sg=0, #default to CBOW. if sg=1 then skip-gram\n",
    "    hs=0, #default, if hs=0 & negative =/= 0 then neg. sampling. if hs=1, hierarchical softmax\n",
    "    negative=5, #5 words selected for negative sampling\n",
    "    size=100, #size of vector\n",
    "    alpha=0.025,\n",
    "    min_count=5, # ignore words with fewer than 20 apearances\n",
    "    iter=5,\n",
    "    seed=42,\n",
    "    cbow_mean=1, #uses mean for CBOW. If it =0 then sums CBOW (provided CBOW not SG)\n",
    "    min_n=3, # min length of char n-grams\n",
    "    max_n=6, # max length of char n-grams. If 0 or less than min_n, this turns into W2V\n",
    "    trim_rule=None, #if you had a rule to trim down vocabulary\n",
    "    workers=3 # default\n",
    ")    \n",
    "\n",
    "    \n",
    "# build the vocabulary\n",
    "base_jobs_model.build_vocab(sentences = jobs_sentences)\n",
    "\n",
    "# train the model\n",
    "base_jobs_model.train(\n",
    "    sentences=jobs_sentences,\n",
    "    epochs=base_jobs_model.epochs,\n",
    "    total_examples=base_jobs_model.corpus_count,\n",
    "    total_words=base_jobs_model.corpus_total_words\n",
    ")\n",
    "\n",
    "print(base_jobs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_resume(selection=-999):\n",
    "    print(\"There are {} resumes\".format(resumes.shape[0]))\n",
    "  \n",
    "    if selection == -999:\n",
    "        selection = np.random.randint(1, resumes.shape[0]+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print ('\\nselected resume is ID #{}'.format(selection))\n",
    "    \n",
    "    # pick the job text and ID associated with the random int\n",
    "    resume_label = resumes.iloc[selection - 1, resumes.columns.get_loc('ID')] #we could grab ID, but this works for non-indexed labels too\n",
    "    resume_text = resumes.iloc[selection - 1 ]['resume'][:500] \n",
    "    \n",
    "    \n",
    "    print('Resume ID is: {}'.format(resume_label))\n",
    "    print('Resume text is (500 chars): {}'.format(resume_text))\n",
    "    \n",
    "    #Convert the sample document into a list and use the infer_vector method to get a vector representation for it\n",
    "    resume_text_to_process = resumes['lemmatized_resume'][selection - 1]\n",
    "    \n",
    "    return resume_text_to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_resume_find_similar_jobs(resume_you_pick):\n",
    "  \n",
    "    #find all distances between chosen job and each resume\n",
    "    temp_distance = []\n",
    "    min_dist = float(\"inf\")\n",
    "    min_index = float(\"inf\")\n",
    "\n",
    "    \n",
    "    for i in range (0, len(jobs_sentences)):\n",
    "        dist = model.wmdistance(resume_you_pick, jobs_sentences[i])\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_index = i\n",
    "        temp_distance.append((dist, i))\n",
    "\n",
    "    #sort list of tuples\n",
    "    temp_distance.sort(key = operator.itemgetter(0))\n",
    "\n",
    "    return temp_distance\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_jobs(list_of_similar_jobs):\n",
    "    \n",
    "    num_similar = 10 #or 10, 20, 25\n",
    "    print('\\nPrinting {} most similar jobs for this candidate...\\n'.format(num_similar))\n",
    "    for i in range(0,num_similar):\n",
    "        print('\\n#{} most similar job'.format(i+1))\n",
    "        print('Job ID from list: {}'.format(list_of_similar_jobs[i][1]))\n",
    "        print('WM Distance: {}'.format(list_of_similar_jobs[i][0]))\n",
    "        print('Job ID from df: {}'.format(jobs.iloc[list_of_similar_jobs[i][1]]['label']))\n",
    "        print('Job title: {}'.format(jobs.iloc[list_of_similar_jobs[i][1]]['Title']))\n",
    "        print('Company: {}'.format(jobs.iloc[list_of_similar_jobs[i][1]]['Company']))\n",
    "        print('Job Description: {}'.format(jobs.iloc[list_of_similar_jobs[i][1]]['JobDescription']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dissimilar_jobs(list_of_similar_jobs):\n",
    "    \n",
    "    num_dissimilar = 10 #or 10, 20, 25\n",
    "    print('\\nPrinting {} most similar jobs for this candidate...\\n'.format(num_dissimilar))\n",
    "    for i in range(0,num_dissimilar):\n",
    "        print('\\n#{} least similar job'.format(i+1))\n",
    "        print('Job ID from list: {}'.format(list_of_similar_jobs[-(1+i)][1]))\n",
    "        print('WM Distance: {}'.format(list_of_similar_jobs[-(1+i)][0]))\n",
    "        print('Job ID from df: {}'.format(jobs.iloc[list_of_similar_jobs[-(1+i)][1]]['label']))\n",
    "        print('Job title: {}'.format(jobs.iloc[list_of_similar_jobs[-(1+i)][1]]['Title']))\n",
    "        print('Company: {}'.format(jobs.iloc[list_of_similar_jobs[-(1+i)][1]]['Company']))\n",
    "        print('Job Description: {}'.format(jobs.iloc[list_of_similar_jobs[-(1+i)][1]]['JobDescription']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1219 resumes\n",
      "\n",
      "selected resume is ID #915\n",
      "Resume ID is: 915\n",
      "Resume text is (500 chars): your resume try determine position would work objective statement says want position offers growth advancement opportunity expand education training tells employer looking education experiencewhat put first education pertinent work seeking employment history list education first education pertinent job may wish list specific course titles completed directly related work would employment history pertinent listed first resume academic background academic background included resume recently obtaine\n"
     ]
    }
   ],
   "source": [
    "chosen_resume = pick_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_job_list = given_resume_find_similar_jobs(chosen_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similar_jobs(ordered_job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dissimilar_jobs(ordered_job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
