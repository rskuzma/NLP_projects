{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastText\n",
    "========\n",
    "- Made by facebook <https://github.com/facebookresearch/fastText>\n",
    "- Treats each word as the aggregation of its subwords. \n",
    "    - Subwords are character n-grams of the word. (e.g. army --> a, r, m, y, ar, rm, my, arm, rmy,\n",
    "- Pros:\n",
    "    - Much better thahn Word2Vec on syntactic tasks, especially with small training corpus\n",
    "    - fastText can be used to obtain vectors for out-of-vocabulary (OOV) words\n",
    "- Cons\n",
    "    - Slightly worse than Word2Vec semantic tasks\n",
    "    - Slower training time than Word2Vec\n",
    "    - Comparision: <https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/Word2Vec_FastText_Comparison.ipynb>\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Credit:\n",
    "- https://radimrehurek.com/gensim/auto_examples/tutorials/run_fasttext.html#sphx-glr-download-auto-examples-tutorials-run-fasttext-py\n",
    "- wm distances work:\n",
    "    - Ofir Pele and Michael Werman “A linear time histogram metric for improved SIFT matching”\n",
    "    - Ofir Pele and Michael Werman “Fast and robust earth mover’s distances”\n",
    "    - Matt Kusner et al. “From Word Embeddings To Document Distances”.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy import spatial\n",
    "\n",
    "from pprint import pprint\n",
    "import operator\n",
    "\n",
    "from gensim.models.fasttext import FastText as FT_gensim\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Resume Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_resume_cleaned_lemmatized_tokenized_path = '/Users/richardkuzma/coding/NLP_projects/job_recommender_project/data/resumes_tokenized_lemmatized.pickle'\n",
    "ec2_path = '/home/ubuntu/NLP_projects/job_recommender_project/data/resumes_tokenized_lemmatized.pickle'\n",
    "\n",
    "# resumes = pd.read_pickle(local_resume_cleaned_lemmatized_tokenized_path)\n",
    "resumes = pd.read_pickle(ec2_path)\n",
    "resumes_sentences = resumes['lemmatized_resume'].tolist()\n",
    "resumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list where each element is a list of strings\n",
    "resumes_sentences[0][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training base FT model for resumes\n",
    "--------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_res_model = FT_gensim(\n",
    "    sentences=None,\n",
    "    sg=0, #default to CBOW. if sg=1 then skip-gram\n",
    "    hs=0, #default, if hs=0 & negative =/= 0 then neg. sampling. if hs=1, hierarchical softmax\n",
    "    negative=5, #5 words selected for negative sampling\n",
    "    size=100, #size of vector\n",
    "    alpha=0.025,\n",
    "    min_count=5, # ignore words with fewer than 20 apearances\n",
    "    iter=5,\n",
    "    seed=42,\n",
    "    cbow_mean=1, #uses mean for CBOW. If it =0 then sums CBOW (provided CBOW not SG)\n",
    "    min_n=3, # min length of char n-grams\n",
    "    max_n=6, # max length of char n-grams. If 0 or less than min_n, this turns into W2V\n",
    "    trim_rule=None, #if you had a rule to trim down vocabulary\n",
    "    workers=3 # default\n",
    ")    \n",
    "\n",
    "    \n",
    "# build the vocabulary\n",
    "base_res_model.build_vocab(sentences = resumes_sentences)\n",
    "\n",
    "# train the model\n",
    "base_res_model.train(\n",
    "    sentences=resumes_sentences,\n",
    "    epochs=base_res_model.epochs,\n",
    "    total_examples=base_res_model.corpus_count,\n",
    "    total_words=base_res_model.corpus_total_words\n",
    ")\n",
    "\n",
    "print(base_res_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_res_model.save('/home/ubuntu/NLP_projects/job_recommender_project/models/fasttext/base_res_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "### Need to be able to pick individual jobs to compare to all resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load jobs into df\n",
    "\n",
    "local_jobs_cleaned_lemmatized_tokenized_path = '/Users/richardkuzma/coding/NLP_projects/job_recommender_project/data/large_files/jobs_tokenized_lemmatized.pickle'\n",
    "ec2_path = '/home/ubuntu/NLP_projects/job_recommender_project/data/large_files/jobs_tokenized_lemmatized.pickle'\n",
    "\n",
    "jobs = pd.read_pickle(ec2_path)\n",
    "# jobs = pd.read_pickle(local_jobs_cleaned_lemmatized_tokenized_path)\n",
    "\n",
    "jobs_list = jobs['lemmatized_combined'].tolist()\n",
    "jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### end experiment zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#13124 total jobs\n",
    "jobs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_resumes(model=base_res_model):\n",
    "    \"\"\"returns list of document vectors \"\"\"\n",
    "    res_vecs = []\n",
    "    for i in range(0, len(resumes_sentences)):\n",
    "        temp_vec = model.wv[resumes_sentences[i]]\n",
    "        one_dim_vec = np.mean(temp_vec, axis=0)\n",
    "        res_vecs.append(one_dim_vec)\n",
    "    return res_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_job(selection=-999):\n",
    "    print(\"There are {} jobs\".format(jobs.shape[0]))\n",
    "    \n",
    "    # Select a random int from 0 to length of rjob set\n",
    "    rand_int = np.random.randint(1, jobs.shape[0]+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if selection == -999:\n",
    "        selection = rand_int\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print ('\\nselected job is ID #{}'.format(selection))\n",
    "    \n",
    "    # pick the job text and ID associated with the random int\n",
    "    job_label = jobs.iloc[selection - 1, jobs.columns.get_loc('label')] #we could grab ID, but this works for non-indexed labels too\n",
    "    job_title = jobs.iloc[selection - 1 ]['Title']\n",
    "    job_company = jobs.iloc[selection - 1 ]['Company']\n",
    "    job_description = jobs.iloc[selection - 1 ]['JobDescription']\n",
    "    \n",
    "    \n",
    "    print('Job Posting ID is: {}'.format(job_label))\n",
    "    print('Job Posting Title: {}'.format(job_title))\n",
    "    print('Job Posting Company: {}'.format(job_company))\n",
    "    print('Job Posting Description: {}'.format(job_description))\n",
    "    \n",
    "    #Convert the sample document into a list and use the infer_vector method to get a vector representation for it\n",
    "    job_text_to_process = jobs['lemmatized_combined'][selection - 1]\n",
    "    \n",
    "    return job_text_to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_job_find_similar_resumes(job_you_pick, model=base_res_model, resumes_vectors = rv):\n",
    "\n",
    "        \n",
    "    #find all distances between chosen job and each resume\n",
    "    temp_distance = []\n",
    "    min_dist = float(\"inf\")\n",
    "    min_index = float(\"inf\")\n",
    "\n",
    "    #turning chosen job into vector\n",
    "    job_temp = model.wv[job_you_pick]\n",
    "    job_vec = np.mean(job_temp, axis=0)\n",
    "    \n",
    "    \n",
    "    for i in range (0, len(resumes_vectors)):\n",
    "        # print(i)\n",
    "        # dist = model.wmdistance(job_you_pick, resumes_sentences[i])\n",
    "        dist = spatial.distance.cosine(job_vec, resumes_vectors[i])\n",
    "        \n",
    "        \n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_index = i\n",
    "        temp_distance.append((dist, i))\n",
    "\n",
    "    #sort list of tuples\n",
    "    temp_distance.sort(key = operator.itemgetter(0))\n",
    "    return temp_distance\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_resumes(list_of_similar_resumes):\n",
    "    num_similar = 10 #or 10, 20, 25\n",
    "    print('\\nPrinting {} MOST similar candidates...\\n'.format(num_similar))\n",
    "    for i in range(0,num_similar):\n",
    "        print('\\n#{} most similar job'.format(i+1))\n",
    "        print('Resume ID from list: {}'.format(list_of_similar_resumes[i][1]))\n",
    "        print('Cosine Distance: {}'.format(list_of_similar_resumes[i][0]))\n",
    "        print('Resume ID from df: {}'.format(resumes.iloc[list_of_similar_resumes[i][1]]['ID']))\n",
    "        print('Resume text (500 chars): {}'.format(resumes.iloc[list_of_similar_resumes[i][1]]['resume'][0:500]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dissimilar_resumes(list_of_similar_resumes):\n",
    "    \n",
    "    \n",
    "    num_dissimilar = 10 #or 10, 20, 25\n",
    "    print('\\nPrinting {} LEAST similar candidate resumes...\\n'.format(num_dissimilar))\n",
    "    count = 0\n",
    "    for i in range(0, len(list_of_similar_resumes)):\n",
    "        if count == num_dissimilar:\n",
    "            break\n",
    "        if resumes.iloc[list_of_similar_resumes[-(1+i)][1]]['resume'] != 'nan':        \n",
    "            print('\\n#{} least similar candidate'.format(count+1))\n",
    "            print('Candidate ID from list: {}'.format(list_of_similar_resumes[-(1+i)][1]))\n",
    "            print('Cosine Distance: {}'.format(list_of_similar_resumes[-(1+i)][0]))\n",
    "            print('Resume ID from df: {}'.format(resumes.iloc[list_of_similar_resumes[-(1+i)][1]]['ID']))\n",
    "            print('Resume text (500 chars): {}'.format(resumes.iloc[list_of_similar_resumes[-(1+i)][1]]['resume'][:500]))\n",
    "            count +=1\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Given a job, find similar candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv = vectorize_resumes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_job = pick_job()\n",
    "# 5773 data analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_resumes_list = given_job_find_similar_resumes(chosen_job)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_res_model.save('/home/ubuntu/NLP_projects/job_recommender_project/models/fasttext/base_res_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_similar_resumes(ordered_resumes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_dissimilar_resumes(ordered_resumes_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model to find jobs for a candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_sentences = jobs['lemmatized_combined'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_jobs_model = FT_gensim(\n",
    "    sentences=None,\n",
    "    sg=0, #default to CBOW. if sg=1 then skip-gram\n",
    "    hs=0, #default, if hs=0 & negative =/= 0 then neg. sampling. if hs=1, hierarchical softmax\n",
    "    negative=5, #5 words selected for negative sampling\n",
    "    size=100, #size of vector\n",
    "    alpha=0.025,\n",
    "    min_count=5, # ignore words with fewer than 20 apearances\n",
    "    iter=5,\n",
    "    seed=42,\n",
    "    cbow_mean=1, #uses mean for CBOW. If it =0 then sums CBOW (provided CBOW not SG)\n",
    "    min_n=3, # min length of char n-grams\n",
    "    max_n=6, # max length of char n-grams. If 0 or less than min_n, this turns into W2V\n",
    "    trim_rule=None, #if you had a rule to trim down vocabulary\n",
    "    workers=3 # default\n",
    ")    \n",
    "\n",
    "    \n",
    "# build the vocabulary\n",
    "base_jobs_model.build_vocab(sentences = jobs_sentences)\n",
    "\n",
    "# train the model\n",
    "base_jobs_model.train(\n",
    "    sentences=jobs_sentences,\n",
    "    epochs=base_jobs_model.epochs,\n",
    "    total_examples=base_jobs_model.corpus_count,\n",
    "    total_words=base_jobs_model.corpus_total_words\n",
    ")\n",
    "\n",
    "print(base_jobs_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_jobs_model.save('/home/ubuntu/NLP_projects/job_recommender_project/models/fasttext/base_jobs_ft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_jobs(model=base_jobs_model):\n",
    "    \"\"\"returns list of document vectors \"\"\"\n",
    "    jobs_vecs = []\n",
    "    for i in range(0, len(jobs_sentences)):\n",
    "        temp_vec = model.wv[jobs_sentences[i]]\n",
    "        one_dim_vec = np.mean(temp_vec, axis=0)\n",
    "        jobs_vecs.append(one_dim_vec)\n",
    "    return jobs_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_resume(selection=-999):\n",
    "    print(\"There are {} resumes\".format(resumes.shape[0]))\n",
    "  \n",
    "    if selection == -999:\n",
    "        selection = np.random.randint(1, resumes.shape[0]+1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print ('\\nselected resume is ID #{}'.format(selection))\n",
    "    \n",
    "    # pick the job text and ID associated with the random int\n",
    "    resume_label = resumes.iloc[selection - 1, resumes.columns.get_loc('ID')] #we could grab ID, but this works for non-indexed labels too\n",
    "    resume_text = resumes.iloc[selection - 1 ]['resume'][:500] \n",
    "    \n",
    "    \n",
    "    print('Resume ID is: {}'.format(resume_label))\n",
    "    print('Resume text is (500 chars): {}'.format(resume_text))\n",
    "    \n",
    "    #Convert the sample document into a list and use the infer_vector method to get a vector representation for it\n",
    "    resume_text_to_process = resumes['lemmatized_resume'][selection - 1]\n",
    "    \n",
    "    return resume_text_to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_resume_find_similar_jobs(resume_you_pick, model=base_jobs_model, jobs_vectors = jv):\n",
    "  \n",
    "    #find all distances between chosen job and each resume\n",
    "    temp_distance = []\n",
    "    min_dist = float(\"inf\")\n",
    "    min_index = float(\"inf\")\n",
    "    \n",
    "    res_temp = model.wv[resume_you_pick]\n",
    "    res_vec = np.mean(res_temp, axis=0)\n",
    "    \n",
    "    for i in range (0, len(jobs_vectors)):\n",
    "        dist = spatial.distance.cosine(res_vec, jobs_vectors[i])\n",
    "#         dist = model.wmdistance(resume_you_pick, jobs_sentences[i])\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_index = i\n",
    "        temp_distance.append((dist, i))\n",
    "\n",
    "\n",
    "                \n",
    "    #sort list of tuples\n",
    "    temp_distance.sort(key = operator.itemgetter(0))\n",
    "\n",
    "    return temp_distance\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar_jobs(list_of_similar_jobs):\n",
    "    \n",
    "    num_similar = 10 #or 10, 20, 25\n",
    "    print('\\nPrinting {} most similar jobs for this candidate...\\n'.format(num_similar))\n",
    "    for i in range(0,num_similar):\n",
    "        print('\\n#{} most similar job'.format(i+1))\n",
    "        print('Job ID from list: {}'.format(list_of_similar_jobs[i][1]))\n",
    "        print('Cosine distance: {}'.format(list_of_similar_jobs[i][0]))\n",
    "        print('Job ID from df: {}'.format(jobs.iloc[list_of_similar_jobs[i][1]]['label']))\n",
    "        print('Job title: {}'.format(jobs.iloc[list_of_similar_jobs[i][1]]['Title']))\n",
    "        print('Company: {}'.format(jobs.iloc[list_of_similar_jobs[i][1]]['Company']))\n",
    "        print('Job Description: {}'.format(jobs.iloc[list_of_similar_jobs[i][1]]['JobDescription']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dissimilar_jobs(list_of_similar_jobs):\n",
    "    \n",
    "    num_dissimilar = 10 #or 10, 20, 25\n",
    "    print('\\nPrinting {} LEAST similar jobs for this candidate...\\n'.format(num_dissimilar))\n",
    "    for i in range(0,num_dissimilar):\n",
    "        print('\\n#{} least similar job'.format(i+1))\n",
    "        print('Job ID from list: {}'.format(list_of_similar_jobs[-(1+i)][1]))\n",
    "        print('Cosine Distance: {}'.format(list_of_similar_jobs[-(1+i)][0]))\n",
    "        print('Job ID from df: {}'.format(jobs.iloc[list_of_similar_jobs[-(1+i)][1]]['label']))\n",
    "        print('Job title: {}'.format(jobs.iloc[list_of_similar_jobs[-(1+i)][1]]['Title']))\n",
    "        print('Company: {}'.format(jobs.iloc[list_of_similar_jobs[-(1+i)][1]]['Company']))\n",
    "        print('Job Description: {}'.format(jobs.iloc[list_of_similar_jobs[-(1+i)][1]]['JobDescription']))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "jv = vectorize_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_resume = pick_resume()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_job_list = given_resume_find_similar_jobs(chosen_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similar_jobs(ordered_job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_dissimilar_jobs(ordered_job_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
