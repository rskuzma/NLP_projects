{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LDA Model\n",
    "=========\n",
    "\n",
    "Introduces Gensim's LDA model and demonstrates its use on the NIPS corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will:\n",
    "\n",
    "* Load data.\n",
    "* Pre-process data.\n",
    "* Transform documents to a vectorized form.\n",
    "* Train an LDA model.\n",
    "\n",
    "If you are not familiar with the LDA model or how to use it in Gensim, I\n",
    "suggest you read up on that before continuing with this tutorial. Basic\n",
    "understanding of the LDA model should suffice. Examples:\n",
    "\n",
    "* `Introduction to Latent Dirichlet Allocation <http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation>`_\n",
    "* Gensim tutorial: `sphx_glr_auto_examples_core_run_topics_and_transformations.py`\n",
    "* Gensim's LDA model API docs: :py:class:`gensim.models.LdaModel`\n",
    "\n",
    "\n",
    "Data: 1740 NIPS papers\n",
    ".. Important::\n",
    "    The corpus contains 1740 documents, and not particularly long ones.\n",
    "    So keep in mind that this tutorial is not geared towards efficiency, and be\n",
    "    careful before applying the code to a large dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We'll try with cleaned job data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HR</td>\n",
       "      <td>john h smith phr    po box  callahan fl  infog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>name surname address mobile noemail personal p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HR</td>\n",
       "      <td>anthony brown hr assistant areas expertise per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>HR</td>\n",
       "      <td>id career objective pursue growth oriented ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>HR</td>\n",
       "      <td>human resources director xefxxbexpert organiza...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Category                                             resume\n",
       "0   1       HR  john h smith phr    po box  callahan fl  infog...\n",
       "1   2       HR  name surname address mobile noemail personal p...\n",
       "2   3       HR  anthony brown hr assistant areas expertise per...\n",
       "3   4       HR   id career objective pursue growth oriented ca...\n",
       "4   5       HR  human resources director xefxxbexpert organiza..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import os.path\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import smart_open\n",
    "\n",
    "\n",
    "\n",
    "clean_jobs_path = '/Users/richardkuzma/coding/NLP_projects/job_recommender_project/data/cleaned_job_posts_madhab.csv'\n",
    "clean_resumes_path = '/Users/richardkuzma/coding/NLP_projects/job_recommender_project/data/cleaned_resume_dataset_maitrip.csv'\n",
    "\n",
    "resumes = pd.read_csv(clean_resumes_path)\n",
    "\n",
    "resumes.head()\n",
    "\n",
    "\n",
    "# def extract_documents(url='https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz'):\n",
    "#     fname = url.split('/')[-1]\n",
    "    \n",
    "#     # Download the file to local storage first.\n",
    "#     # We can't read it on the fly because of \n",
    "#     # https://github.com/RaRe-Technologies/smart_open/issues/331\n",
    "#     if not os.path.isfile(fname):\n",
    "#         with smart_open.open(url, \"rb\") as fin:\n",
    "#             with smart_open.open(fname, 'wb') as fout:\n",
    "#                 while True:\n",
    "#                     buf = fin.read(io.DEFAULT_BUFFER_SIZE)\n",
    "#                     if not buf:\n",
    "#                         break\n",
    "#                     fout.write(buf)\n",
    "                         \n",
    "#     with tarfile.open(fname, mode='r:gz') as tar:\n",
    "#         # Ignore directory entries, as well as files like README, etc.\n",
    "#         files = [\n",
    "#             m for m in tar.getmembers()\n",
    "#             if m.isfile() and re.search(r'nipstxt/nips\\d+/\\d+\\.txt', m.name)\n",
    "#         ]\n",
    "#         for member in sorted(files, key=lambda x: x.name):\n",
    "#             member_bytes = tar.extractfile(member).read()\n",
    "#             yield member_bytes.decode('utf-8', errors='replace')\n",
    "\n",
    "# docs = list(extract_documents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a list of 1740 documents, where each document is a Unicode string. \n",
    "If you're thinking about using your own corpus, then you need to make sure\n",
    "that it's in the same format (list of Unicode strings) before proceeding\n",
    "with the rest of this tutorial.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1219\n",
      "john h smith phr    po box  callahan fl  infogreatresumesfastcom approachable innovator passion human resources senior human resources professional personable analytical flexible senior hr professional multifaceted expertise seasoned benefits administrator extensive experience working highly paid professionals client relationship based settings dynamic team leader capable analyzing alternatives identifying tough choices communicating total value benefit compensation packages senior level executi\n"
     ]
    }
   ],
   "source": [
    "print(len(resumes))\n",
    "print(resumes['resume'][0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process and vectorize the documents\n",
    "---------------------------------------\n",
    "\n",
    "As part of preprocessing, we will:\n",
    "\n",
    "* Tokenize (split the documents into tokens).\n",
    "* Lemmatize the tokens.\n",
    "* Compute bigrams.\n",
    "* Compute a bag-of-words representation of the data.\n",
    "\n",
    "First we tokenize the text using a regular expression tokenizer from NLTK. We\n",
    "remove numeric tokens and tokens that are only a single character, as they\n",
    "don't tend to be useful, and the dataset contains a lot of them.\n",
    "\n",
    ".. Important::\n",
    "\n",
    "   This tutorial uses the nltk library for preprocessing, although you can\n",
    "   replace it with something else if you want.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "import nltk\n",
    "\n",
    "#remove numbers\n",
    "resumes['resume'].str.replace('\\d+', '')\n",
    "\n",
    "#didn't remove these in the cleaning_data notebook\n",
    "resumes['resume'].str.replace('xefxxb', ' ')\n",
    "resumes['resume'].str.replace('xexxa', ' ')\n",
    "resumes['tokenized_resume'] = resumes.apply(lambda row: nltk.word_tokenize(str(row['resume'])), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>resume</th>\n",
       "      <th>tokenized_resume</th>\n",
       "      <th>lemmatized_resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HR</td>\n",
       "      <td>john h smith phr    po box  callahan fl  infog...</td>\n",
       "      <td>[john, h, smith, phr, po, box, callahan, fl, i...</td>\n",
       "      <td>[john, h, smith, phr, po, box, callahan, fl, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>name surname address mobile noemail personal p...</td>\n",
       "      <td>[name, surname, address, mobile, noemail, pers...</td>\n",
       "      <td>[name, surname, address, mobile, noemail, pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HR</td>\n",
       "      <td>anthony brown hr assistant areas expertise per...</td>\n",
       "      <td>[anthony, brown, hr, assistant, areas, experti...</td>\n",
       "      <td>[anthony, brown, hr, assistant, area, expertis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>HR</td>\n",
       "      <td>id career objective pursue growth oriented ca...</td>\n",
       "      <td>[id, career, objective, pursue, growth, orient...</td>\n",
       "      <td>[id, career, objective, pursue, growth, orient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>HR</td>\n",
       "      <td>human resources director xefxxbexpert organiza...</td>\n",
       "      <td>[human, resources, director, xefxxbexpert, org...</td>\n",
       "      <td>[human, resource, director, xefxxbexpert, orga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Category                                             resume  \\\n",
       "0   1       HR  john h smith phr    po box  callahan fl  infog...   \n",
       "1   2       HR  name surname address mobile noemail personal p...   \n",
       "2   3       HR  anthony brown hr assistant areas expertise per...   \n",
       "3   4       HR   id career objective pursue growth oriented ca...   \n",
       "4   5       HR  human resources director xefxxbexpert organiza...   \n",
       "\n",
       "                                    tokenized_resume  \\\n",
       "0  [john, h, smith, phr, po, box, callahan, fl, i...   \n",
       "1  [name, surname, address, mobile, noemail, pers...   \n",
       "2  [anthony, brown, hr, assistant, areas, experti...   \n",
       "3  [id, career, objective, pursue, growth, orient...   \n",
       "4  [human, resources, director, xefxxbexpert, org...   \n",
       "\n",
       "                                   lemmatized_resume  \n",
       "0  [john, h, smith, phr, po, box, callahan, fl, i...  \n",
       "1  [name, surname, address, mobile, noemail, pers...  \n",
       "2  [anthony, brown, hr, assistant, area, expertis...  \n",
       "3  [id, career, objective, pursue, growth, orient...  \n",
       "4  [human, resource, director, xefxxbexpert, orga...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "john   0\n",
      "h   1\n",
      "smith   2\n",
      "phr   3\n",
      "po   4\n",
      "box   5\n",
      "callahan   6\n",
      "fl   7\n",
      "infogreatresumesfastcom   8\n",
      "approachable   9\n",
      "innovator   10\n",
      "passion   11\n",
      "human   12\n",
      "resources   13\n",
      "senior   14\n",
      "human   15\n",
      "resources   16\n",
      "professional   17\n",
      "personable   18\n",
      "analytical   19\n",
      "flexible   20\n",
      "senior   21\n",
      "hr   22\n",
      "professional   23\n",
      "multifaceted   24\n",
      "expertise   25\n",
      "seasoned   26\n",
      "benefits   27\n",
      "administrator   28\n",
      "extensive   29\n",
      "experience   30\n",
      "working   31\n",
      "highly   32\n",
      "paid   33\n",
      "professionals   34\n",
      "client   35\n",
      "relationship   36\n",
      "based   37\n",
      "settings   38\n",
      "dynamic   39\n",
      "team   40\n",
      "leader   41\n",
      "capable   42\n",
      "analyzing   43\n",
      "alternatives   44\n",
      "identifying   45\n",
      "tough   46\n",
      "choices   47\n",
      "communicating   48\n",
      "total   49\n",
      "value   50\n",
      "benefit   51\n",
      "compensation   52\n",
      "packages   53\n",
      "senior   54\n",
      "level   55\n",
      "executives   56\n",
      "employees   57\n",
      "core   58\n",
      "competencies   59\n",
      "benefits   60\n",
      "administration   61\n",
      "customer   62\n",
      "service   63\n",
      "cost   64\n",
      "control   65\n",
      "recruiting   66\n",
      "acquisition   67\n",
      "management   68\n",
      "compliance   69\n",
      "reporting   70\n",
      "retention   71\n",
      "professional   72\n",
      "services   73\n",
      "domestic   74\n",
      "international   75\n",
      "benefits   76\n",
      "collaboration   77\n",
      "adaptability   78\n",
      "change   79\n",
      "management   80\n",
      "defined   81\n",
      "contribution   82\n",
      "plans   83\n",
      "auditing   84\n",
      "negotiation   85\n",
      "corporate   86\n",
      "hr   87\n",
      "policies   88\n",
      "full   89\n",
      "lifecycle   90\n",
      "training   91\n",
      "k   92\n",
      "form   93\n",
      "confidential   94\n",
      "files   95\n",
      "eeo   96\n",
      "aap   97\n",
      "fmla   98\n",
      "std   99\n",
      "ltd   100\n",
      "h   101\n",
      "b   102\n",
      "visa   103\n",
      "vets   104\n",
      "eap   105\n",
      "processing   106\n",
      "payroll   107\n",
      "hr   108\n",
      "technology   109\n",
      "hris   110\n",
      "data   111\n",
      "management   112\n",
      "auditing   113\n",
      "ultipro   114\n",
      "back   115\n",
      "office   116\n",
      "ultipro   117\n",
      "web   118\n",
      "connect   119\n",
      "deltek   120\n",
      "costpoint   121\n",
      "deltek   122\n",
      "gcs   123\n",
      "premiere   124\n",
      "cognos   125\n",
      "adp   126\n",
      "professional   127\n",
      "experience   128\n",
      "human   129\n",
      "services   130\n",
      "inc   131\n",
      "tampa   132\n",
      "fl   133\n",
      "present   134\n",
      "providing   135\n",
      "institutional   136\n",
      "behavioral   137\n",
      "health   138\n",
      "medical   139\n",
      "management   140\n",
      "contracting   141\n",
      "services   142\n",
      "employees   143\n",
      "benefits   144\n",
      "manager   145\n",
      "manage   146\n",
      "administer   147\n",
      "robust   148\n",
      "m   149\n",
      "health   150\n",
      "welfare   151\n",
      "program   152\n",
      "administer   153\n",
      "benefits   154\n",
      "programs   155\n",
      "two   156\n",
      "smaller   157\n",
      "minority   158\n",
      "business   159\n",
      "enterprise   160\n",
      "clients   161\n",
      "deliver   162\n",
      "excellence   163\n",
      "client   164\n",
      "services   165\n",
      "leveraging   166\n",
      "interpersonal   167\n",
      "skills   168\n",
      "establish   169\n",
      "rapport   170\n",
      "team   171\n",
      "members   172\n",
      "perform   173\n",
      "cost   174\n",
      "analysis   175\n",
      "gather   176\n",
      "claims   177\n",
      "data   178\n",
      "coordinate   179\n",
      "enrollment   180\n",
      "large   181\n",
      "selffunded   182\n",
      "medical   183\n",
      "plan   184\n",
      "plus   185\n",
      "two   186\n",
      "smaller   187\n",
      "fully   188\n",
      "insured   189\n",
      "plans   190\n",
      "analyze   191\n",
      "data   192\n",
      "projections   193\n",
      "create   194\n",
      "forecasts   195\n",
      "narrative   196\n",
      "graph   197\n",
      "format   198\n",
      "clearly   199\n",
      "succinctly   200\n",
      "communicate   201\n",
      "trends   202\n",
      "senior   203\n",
      "management   204\n",
      "meetings   205\n",
      "directly   206\n",
      "supervise   207\n",
      "staff   208\n",
      "hr   209\n",
      "admin   210\n",
      "plus   211\n",
      "two   212\n",
      "human   213\n",
      "resource   214\n",
      "specialists   215\n",
      "lead   216\n",
      "healthcare   217\n",
      "reform   218\n",
      "change   219\n",
      "management   220\n",
      "process   221\n",
      "analyzing   222\n",
      "changes   223\n",
      "developing   224\n",
      "strategic   225\n",
      "plans   226\n",
      "timeline   227\n",
      "crucial   228\n",
      "decisions   229\n",
      "ahead   230\n",
      "deadlines   231\n",
      "communicate   232\n",
      "early   233\n",
      "changes   234\n",
      "dependent   235\n",
      "coverage   236\n",
      "preventive   237\n",
      "care   238\n",
      "lifetime   239\n",
      "maximum   240\n",
      "benefits   241\n",
      "employees   242\n",
      "identified   243\n",
      "researched   244\n",
      "implemented   245\n",
      "hsa   246\n",
      "healthcare   247\n",
      "savings   248\n",
      "account   249\n",
      "health   250\n",
      "insurance   251\n",
      "option   252\n",
      "high   253\n",
      "deductible   254\n",
      "health   255\n",
      "insurance   256\n",
      "plan   257\n",
      "provide   258\n",
      "tax   259\n",
      "sheltered   260\n",
      "benefits   261\n",
      "physicians   262\n",
      "high   263\n",
      "income   264\n",
      "highprofile   265\n",
      "professionals   266\n",
      "saved   267\n",
      "k   268\n",
      "annually   269\n",
      "restructuring   270\n",
      "open   271\n",
      "enrollment   272\n",
      "strategy   273\n",
      "leveraging   274\n",
      "existing   275\n",
      "human   276\n",
      "capital   277\n",
      "hr   278\n",
      "business   279\n",
      "partners   280\n",
      "achieve   281\n",
      "better   282\n",
      "communication   283\n",
      "lower   284\n",
      "cost   285\n",
      "quickly   286\n",
      "establish   287\n",
      "rapport   288\n",
      "employees   289\n",
      "balance   290\n",
      "client   291\n",
      "relationship   292\n",
      "needs   293\n",
      "organizational   294\n",
      "goals   295\n",
      "cost   296\n",
      "controls   297\n",
      "compliance   298\n",
      "requirements   299\n",
      "create   300\n",
      "positive   301\n",
      "work   302\n",
      "environment   303\n",
      "maximize   304\n",
      "employee   305\n",
      "recruiting   306\n",
      "retention   307\n",
      "implementing   308\n",
      "competitive   309\n",
      "benefits   310\n",
      "packages   311\n",
      "clearly   312\n",
      "communicating   313\n",
      "value   314\n",
      "team   315\n",
      "members   316\n",
      "reduced   317\n",
      "costs   318\n",
      "k   319\n",
      "per   320\n",
      "year   321\n",
      "collaborating   322\n",
      "closely   323\n",
      "legal   324\n",
      "department   325\n",
      "create   326\n",
      "house   327\n",
      "total   328\n",
      "comp   329\n",
      "statement   330\n",
      "including   331\n",
      "implications   332\n",
      "taxes   333\n",
      "social   334\n",
      "security   335\n",
      "produced   336\n",
      "better   337\n",
      "quality   338\n",
      "analysis   339\n",
      "lower   340\n",
      "costs   341\n",
      "xcjohn   342\n",
      "h   343\n",
      "smith   344\n",
      "phr   345\n",
      "page   346\n",
      "infogreatresumesfastcom   347\n",
      "approachable   348\n",
      "innovator   349\n",
      "passion   350\n",
      "human   351\n",
      "resources   352\n",
      "human   353\n",
      "management   354\n",
      "inc   355\n",
      "atlanta   356\n",
      "ga   357\n",
      "nonprofit   358\n",
      "government   359\n",
      "consulting   360\n",
      "firm   361\n",
      "domestic   362\n",
      "international   363\n",
      "employees   364\n",
      "senior   365\n",
      "human   366\n",
      "resources   367\n",
      "generalist   368\n",
      "performed   369\n",
      "diverse   370\n",
      "human   371\n",
      "resources   372\n",
      "functions   373\n",
      "administered   374\n",
      "benefits   375\n",
      "led   376\n",
      "special   377\n",
      "projects   378\n",
      "updated   379\n",
      "policies   380\n",
      "collaborated   381\n",
      "eeo   382\n",
      "vets   383\n",
      "affirmative   384\n",
      "action   385\n",
      "plan   386\n",
      "aap   387\n",
      "reporting   388\n",
      "revamped   389\n",
      "orientation   390\n",
      "training   391\n",
      "programs   392\n",
      "newly   393\n",
      "hired   394\n",
      "employees   395\n",
      "include   396\n",
      "comprehensive   397\n",
      "information   398\n",
      "delivered   399\n",
      "personal   400\n",
      "formats   401\n",
      "teleconferences   402\n",
      "small   403\n",
      "group   404\n",
      "meetings   405\n",
      "tours   406\n",
      "one   407\n",
      "one   408\n",
      "conversations   409\n",
      "follow   410\n",
      "phone   411\n",
      "calls   412\n",
      "supervised   413\n",
      "hr   414\n",
      "assistant   415\n",
      "temporary   416\n",
      "human   417\n",
      "resource   418\n",
      "specialists   419\n",
      "handling   420\n",
      "high   421\n",
      "volume   422\n",
      "work   423\n",
      "peak   424\n",
      "periods   425\n",
      "open   426\n",
      "enrollment   427\n",
      "company   428\n",
      "merger   429\n",
      "led   430\n",
      "change   431\n",
      "management   432\n",
      "process   433\n",
      "seamlessly   434\n",
      "integrated   435\n",
      "benefits   436\n",
      "compensation   437\n",
      "retirement   438\n",
      "plans   439\n",
      "logistics   440\n",
      "newly   441\n",
      "acquired   442\n",
      "company   443\n",
      "employees   444\n",
      "prepared   445\n",
      "encompassing   446\n",
      "reports   447\n",
      "total   448\n",
      "compensation   449\n",
      "plans   450\n",
      "developed   451\n",
      "webinars   452\n",
      "personal   453\n",
      "meetings   454\n",
      "communicate   455\n",
      "value   456\n",
      "benefits   457\n",
      "employees   458\n",
      "despite   459\n",
      "reductions   460\n",
      "economic   461\n",
      "recession   462\n",
      "maintained   463\n",
      "employee   464\n",
      "retention   465\n",
      "job   466\n",
      "satisfaction   467\n",
      "rankings   468\n",
      "despite   469\n",
      "cutbacks   470\n",
      "reduced   471\n",
      "overall   472\n",
      "benefit   473\n",
      "costs   474\n",
      "including   475\n",
      "decrease   476\n",
      "std   477\n",
      "benefits   478\n",
      "base   479\n",
      "compensation   480\n",
      "without   481\n",
      "significant   482\n",
      "increase   483\n",
      "employee   484\n",
      "turnover   485\n",
      "researched   486\n",
      "procured   487\n",
      "benefits   488\n",
      "government   489\n",
      "contractors   490\n",
      "traveling   491\n",
      "hazardous   492\n",
      "areas   493\n",
      "including   494\n",
      "kidnap   495\n",
      "ransom   496\n",
      "kr   497\n",
      "increased   498\n",
      "life   499\n",
      "insurance   500\n",
      "travel   501\n",
      "insurance   502\n",
      "identified   503\n",
      "implemented   504\n",
      "medical   505\n",
      "dental   506\n",
      "benefit   507\n",
      "plans   508\n",
      "ensure   509\n",
      "comparable   510\n",
      "compensation   511\n",
      "international   512\n",
      "employees   513\n",
      "worldwide   514\n",
      "managed   515\n",
      "diverse   516\n",
      "retirement   517\n",
      "plans   518\n",
      "including   519\n",
      "defined   520\n",
      "contribution   521\n",
      "b   522\n",
      "a   523\n",
      "executive   524\n",
      "b   525\n",
      "plans   526\n",
      "abc   527\n",
      "corporation   528\n",
      "new   529\n",
      "york   530\n",
      "ny   531\n",
      "engineering   532\n",
      "construction   533\n",
      "consulting   534\n",
      "international   535\n",
      "domestic   536\n",
      "employees   537\n",
      "senior   538\n",
      "human   539\n",
      "resources   540\n",
      "generalist   541\n",
      "acme   542\n",
      "inc   543\n",
      "washington   544\n",
      "dc   545\n",
      "professional   546\n",
      "services   547\n",
      "consulting   548\n",
      "firm   549\n",
      "employees   550\n",
      "hr   551\n",
      "generalist   552\n",
      "benefits   553\n",
      "specialist   554\n",
      "human   555\n",
      "resource   556\n",
      "corporation   557\n",
      "tampa   558\n",
      "fl   559\n",
      "environmental   560\n",
      "management   561\n",
      "consulting   562\n",
      "firm   563\n",
      "employees   564\n",
      "human   565\n",
      "resources   566\n",
      "assistant   567\n",
      "education   568\n",
      "certification   569\n",
      "affiliation   570\n",
      "bachelor   571\n",
      "science   572\n",
      "psychology   573\n",
      "community   574\n",
      "college   575\n",
      "professional   576\n",
      "human   577\n",
      "resources   578\n",
      "phr   579\n",
      "certificate   580\n",
      "employee   581\n",
      "benefit   582\n",
      "specialist   583\n",
      "cebs   584\n",
      "certification   585\n",
      "pending   586\n",
      "member   587\n",
      "shrm   588\n",
      "society   589\n",
      "human   590\n",
      "resource   591\n",
      "management   592\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in resumes['tokenized_resume'][0]:\n",
    "    print(i, \" \", count)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the WordNet lemmatizer from NLTK. A lemmatizer is preferred over a\n",
    "stemmer in this case because it produces more readable words. Output that is\n",
    "easy to read is very desirable in topic modelling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# return [lemmatizer.lemmatize(token) for token in series] for doc in docs]\n",
    "\n",
    "# w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "# lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(pandas_series):\n",
    "    return [lemmatizer.lemmatize(token) for token in pandas_series]\n",
    "\n",
    "resumes['lemmatized_resume'] = resumes['tokenized_resume'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>resume</th>\n",
       "      <th>tokenized_resume</th>\n",
       "      <th>lemmatized_resume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>HR</td>\n",
       "      <td>john h smith phr    po box  callahan fl  infog...</td>\n",
       "      <td>[john, h, smith, phr, po, box, callahan, fl, i...</td>\n",
       "      <td>[john, h, smith, phr, po, box, callahan, fl, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>name surname address mobile noemail personal p...</td>\n",
       "      <td>[name, surname, address, mobile, noemail, pers...</td>\n",
       "      <td>[name, surname, address, mobile, noemail, pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>HR</td>\n",
       "      <td>anthony brown hr assistant areas expertise per...</td>\n",
       "      <td>[anthony, brown, hr, assistant, areas, experti...</td>\n",
       "      <td>[anthony, brown, hr, assistant, area, expertis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>HR</td>\n",
       "      <td>id career objective pursue growth oriented ca...</td>\n",
       "      <td>[id, career, objective, pursue, growth, orient...</td>\n",
       "      <td>[id, career, objective, pursue, growth, orient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>HR</td>\n",
       "      <td>human resources director xefxxbexpert organiza...</td>\n",
       "      <td>[human, resources, director, xefxxbexpert, org...</td>\n",
       "      <td>[human, resource, director, xefxxbexpert, orga...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Category                                             resume  \\\n",
       "0   1       HR  john h smith phr    po box  callahan fl  infog...   \n",
       "1   2       HR  name surname address mobile noemail personal p...   \n",
       "2   3       HR  anthony brown hr assistant areas expertise per...   \n",
       "3   4       HR   id career objective pursue growth oriented ca...   \n",
       "4   5       HR  human resources director xefxxbexpert organiza...   \n",
       "\n",
       "                                    tokenized_resume  \\\n",
       "0  [john, h, smith, phr, po, box, callahan, fl, i...   \n",
       "1  [name, surname, address, mobile, noemail, pers...   \n",
       "2  [anthony, brown, hr, assistant, areas, experti...   \n",
       "3  [id, career, objective, pursue, growth, orient...   \n",
       "4  [human, resources, director, xefxxbexpert, org...   \n",
       "\n",
       "                                   lemmatized_resume  \n",
       "0  [john, h, smith, phr, po, box, callahan, fl, i...  \n",
       "1  [name, surname, address, mobile, noemail, pers...  \n",
       "2  [anthony, brown, hr, assistant, area, expertis...  \n",
       "3  [id, career, objective, pursue, growth, orient...  \n",
       "4  [human, resource, director, xefxxbexpert, orga...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find bigrams in the documents. Bigrams are sets of two adjacent words.\n",
    "Using bigrams we can get phrases like \"machine_learning\" in our output\n",
    "(spaces are replaced with underscores); without bigrams we would only get\n",
    "\"machine\" and \"learning\".\n",
    "\n",
    "Note that in the code below, we find bigrams and then add them to the\n",
    "original data, because we would like to keep the words \"machine\" and\n",
    "\"learning\" as well as the bigram \"machine_learning\".\n",
    "\n",
    ".. Important::\n",
    "    Computing n-grams of large dataset can be very computationally\n",
    "    and memory intensive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=20)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove rare words and common words based on their *document frequency*.\n",
    "Below we remove words that appear in less than 20 documents or in more than\n",
    "50% of the documents. Consider trying to remove words only based on their\n",
    "frequency, or maybe combining that with this approach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we transform the documents to a vectorized form. We simply compute\n",
    "the frequency of each word, including the bigrams.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many tokens and documents we have to train on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "--------\n",
    "\n",
    "We are ready to train the LDA model. We will first discuss how to set some of\n",
    "the training parameters.\n",
    "\n",
    "First of all, the elephant in the room: how many topics do I need? There is\n",
    "really no easy answer for this, it will depend on both your data and your\n",
    "application. I have used 10 topics here because I wanted to have a few topics\n",
    "that I could interpret and \"label\", and because that turned out to give me\n",
    "reasonably good results. You might not need to interpret all your topics, so\n",
    "you could use a large number of topics, for example 100.\n",
    "\n",
    "``chunksize`` controls how many documents are processed at a time in the\n",
    "training algorithm. Increasing chunksize will speed up training, at least as\n",
    "long as the chunk of documents easily fit into memory. I've set ``chunksize =\n",
    "2000``, which is more than the amount of documents, so I process all the\n",
    "data in one go. Chunksize can however influence the quality of the model, as\n",
    "discussed in Hoffman and co-authors [2], but the difference was not\n",
    "substantial in this case.\n",
    "\n",
    "``passes`` controls how often we train the model on the entire corpus.\n",
    "Another word for passes might be \"epochs\". ``iterations`` is somewhat\n",
    "technical, but essentially it controls how often we repeat a particular loop\n",
    "over each document. It is important to set the number of \"passes\" and\n",
    "\"iterations\" high enough.\n",
    "\n",
    "I suggest the following way to choose iterations and passes. First, enable\n",
    "logging (as described in many Gensim tutorials), and set ``eval_every = 1``\n",
    "in ``LdaModel``. When training the model look for a line in the log that\n",
    "looks something like this::\n",
    "\n",
    "   2016-06-21 15:40:06,753 - gensim.models.ldamodel - DEBUG - 68/1566 documents converged within 400 iterations\n",
    "\n",
    "If you set ``passes = 20`` you will see this line 20 times. Make sure that by\n",
    "the final passes, most of the documents have converged. So you want to choose\n",
    "both passes and iterations to be high enough for this to happen.\n",
    "\n",
    "We set ``alpha = 'auto'`` and ``eta = 'auto'``. Again this is somewhat\n",
    "technical, but essentially we are automatically learning two parameters in\n",
    "the model that we usually would have to specify explicitly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the topic coherence of each topic. Below we display the\n",
    "average topic coherence and print the topics in order of topic coherence.\n",
    "\n",
    "Note that we use the \"Umass\" topic coherence measure here (see\n",
    ":py:func:`gensim.models.ldamodel.LdaModel.top_topics`), Gensim has recently\n",
    "obtained an implementation of the \"AKSW\" topic coherence measure (see\n",
    "accompanying blog post, http://rare-technologies.com/what-is-topic-coherence/).\n",
    "\n",
    "If you are familiar with the subject of the articles in this dataset, you can\n",
    "see that the topics below make a lot of sense. However, they are not without\n",
    "flaws. We can see that there is substantial overlap between some topics,\n",
    "others are hard to interpret, and most of them have at least some terms that\n",
    "seem out of place. If you were able to do better, feel free to share your\n",
    "methods on the blog at http://rare-technologies.com/lda-training-tips/ !\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to experiment with\n",
    "-------------------------\n",
    "\n",
    "* ``no_above`` and ``no_below`` parameters in ``filter_extremes`` method.\n",
    "* Adding trigrams or even higher order n-grams.\n",
    "* Consider whether using a hold-out set or cross-validation is the way to go for you.\n",
    "* Try other datasets.\n",
    "\n",
    "Where to go from here\n",
    "---------------------\n",
    "\n",
    "* Check out a RaRe blog post on the AKSW topic coherence measure (http://rare-technologies.com/what-is-topic-coherence/).\n",
    "* pyLDAvis (https://pyldavis.readthedocs.io/en/latest/index.html).\n",
    "* Read some more Gensim tutorials (https://github.com/RaRe-Technologies/gensim/blob/develop/tutorials.md#tutorials).\n",
    "* If you haven't already, read [1] and [2] (see references).\n",
    "\n",
    "References\n",
    "----------\n",
    "\n",
    "1. \"Latent Dirichlet Allocation\", Blei et al. 2003.\n",
    "2. \"Online Learning for Latent Dirichlet Allocation\", Hoffman et al. 2010.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
