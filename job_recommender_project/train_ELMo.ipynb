{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### to load elmo to local disk:\n",
    "#download the model to local so it can be used again and again\n",
    "$ mkdir module/module_elmo2\n",
    "# Download the module, and uncompress it to the destination folder. \n",
    "$ curl -L \"https://tfhub.dev/google/elmo/2?tf-hub-format=compressed\" | tar -zxvC module/module_elmo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#NLP\n",
    "#import spacy\n",
    "import re\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# file loading\n",
    "import pickle\n",
    "\n",
    "#deep learning\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a Tesla V100 with a P3, K80 with P2 \n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resumes only for now\n",
    "# gives df with list of strings (tokenized) as well as lemmatized list of strings\n",
    "\n",
    "ec2_resumes_path = '/home/ubuntu/NLP_projects/job_recommender_project/data/large_files/lf_cleaned_lemmatized_tokenized_resumes.csv'\n",
    "ec2_pickle_resumes_path = '/home/ec2-user/NLP_projects/job_recommender_project/data/resumes_with_list_of_list.pickle'\n",
    "\n",
    "resumes = pd.read_pickle(ec2_pickle_resumes_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resumes['lol'][0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes['los'] = resumes['lol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resumes['los'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, len(resumes['los'])):\n",
    "    for j in range (0, len(resumes['los'][i])):\n",
    "        resumes['los'][i][j] = ' '.join(resumes['los'][i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resumes['los'][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_in = resumes['los']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the lenghts of each resume and resume ID for reference\n",
    "\n",
    "# for x in range(len(elmo_in)):\n",
    "#     print(x, ':  ', len(elmo_in[x]))\n",
    "\n",
    "#     #elmo_in[28] has 212 sentences\n",
    "#     #\n",
    "#     #elmo_in[34] has 945 sentences\n",
    "#     #elmo_in[75] has 3 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's examine resume length in sentences\n",
    "\n",
    "\n",
    "lengths = []\n",
    "for i in range(0, len(elmo_in)):\n",
    "    lengths.extend([len(elmo_in[i])])\n",
    "\n",
    "#lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at distributions of resume length\n",
    "\n",
    "plt.hist(lengths, bins=range(0, 1000, 10))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lengths, bins=range(0, 200, 5))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"example of a short resume:\")\n",
    "print(\"resume number 244\")\n",
    "print(\"resume length: {} sentences\".format(len(elmo_in[244])))\n",
    "print(elmo_in[244])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"example of a medium resume:\")\n",
    "print(\"resume number 202\")\n",
    "print(\"resume length: {} sentences\".format(len(elmo_in[202])))\n",
    "print(elmo_in[202])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"example of a long resume:\")\n",
    "print(\"resume number 28\")\n",
    "print(\"resume length: {} sentences\".format(len(elmo_in[28])))\n",
    "print(elmo_in[28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('min: {}'.format(np.amin(lengths)))\n",
    "print('max: {}'.format(np.amax(lengths)))\n",
    "print('median: {}'.format(np.median(lengths)))\n",
    "print('mean: {}'.format(np.mean(lengths)))\n",
    "print('stdev: {}'.format(np.std(lengths)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_copy = lengths.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_in_smalls = elmo_in.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_in_smalls = elmo_in_smalls.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(elmo_in_smalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in elmo_in_smalls:\n",
    "    if len(i) > 100:\n",
    "        elmo_in_smalls.remove(i)\n",
    "    if len(i) <2:\n",
    "        elmo_in_smalls.remove(i)\n",
    "\n",
    "#elmo_in_smalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[0] ',elmo_in_smalls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[0][3] ', type(elmo_in_smalls[0][3]), len(elmo_in_smalls[0][3]), elmo_in_smalls[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(elmo_in_smalls)):\n",
    "    for j in range (0, len(elmo_in_smalls[i])):\n",
    "        if len(elmo_in_smalls[i][j])<2:\n",
    "            print(i, j, elmo_in_smalls[i][j])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elmo_in_smalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_smalls = []\n",
    "for i in range(0, len(elmo_in_smalls)):\n",
    "    lengths_smalls.extend([len(elmo_in_smalls[i])])\n",
    "\n",
    "#lengths_smalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at distributions of resume length\n",
    "\n",
    "plt.hist(lengths_smalls, bins=range(0, 100, 5))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have lengths_smalls for each length\n",
    "# now we have elmo_in_smalls for each resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(elmo_in_smalls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have elmo saved locally\n",
    "elmo = hub.Module(\"/home/ec2-user/module/module_elmo2\", trainable=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elmo_in_smalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SAVING ELMO_IN_SMALLS\n",
    "type(elmo_in_smalls)\n",
    "# save elmo in smalls\n",
    "np_elmo_in_smalls = np.asarray(elmo_in_smalls)\n",
    "np.save('elmo_resumes_under_100_sentences', np_elmo_in_smalls) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first_try = []\n",
    "# for i in range(0, len(elmo_in_smalls[0])):\n",
    "#     first_try = elmo_vectors(elmo_in_smalls[0][i])\n",
    "#     print(first_try)\n",
    "\n",
    "\n",
    "#trying on elmo_in_smalls[0]\n",
    "embeddings = elmo(elmo_in_smalls[0], signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "print(\"embeddings is type {} and shape {}\".format(type(embeddings), embeddings.shape))\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings_2d = session.run(tf.reduce_mean(embeddings,axis=0))\n",
    "    print(\"message_embeddings_2d is type {} and shape {}\".format(type(message_embeddings_2d), message_embeddings_2d.shape))\n",
    "    message_embeddings_1d = tf.reduce_mean(tf.convert_to_tensor(message_embeddings_2d), axis = 0, keepdims=True)\n",
    "    print(\"message_embeddings_1d is type {} and shape {}\".format(type(message_embeddings_1d), message_embeddings_1d.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = tf.dtypes.cast(embeddings, tf.float16)  # [1, 2], dtype=tf.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:  print(message_embeddings_1d.eval()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying on elmo_in_smalls[1]\n",
    "embeddings1 = elmo(elmo_in_smalls[1], signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "print(\"embeddings is type {} and shape {}\".format(type(embeddings1), embeddings1.shape))\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings1_2d = session.run(tf.reduce_mean(embeddings1,axis=0))\n",
    "    print(\"message_embeddings1_2d is type {} and shape {}\".format(type(message_embeddings1_2d), message_embeddings1_2d.shape))\n",
    "    message_embeddings1_1d = session.run(tf.reduce_mean(tf.convert_to_tensor(message_embeddings1_2d), axis = 0, keepdims=True))\n",
    "    print(\"message_embeddings1_1d is type {} and shape {}\".format(type(message_embeddings1_1d), message_embeddings1_1d.shape))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(message_embeddings1_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:  print(message_embeddings1_1d.eval()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess: print(cosine_similarity(message_embeddings_1d.eval(), message_embeddings1_1d.eval()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"embeddings shape: {}\".format(embeddings.shape))\n",
    "# print(\"elmo_in_smalls[0] sentences: {}\".format(len(elmo_in_smalls[0])))\n",
    "# max_sentence_length = 0\n",
    "# for i in range(0, len(elmo_in_smalls[0])):\n",
    "#     if len(elmo_in_smalls[0][i].split()) > max_sentence_length:\n",
    "#         max_sentence_length = len(elmo_in_smalls[0][i])\n",
    "                                  \n",
    "# print(\"elmo_in_smalls[0] max sentence length: {}\".format(max_sentence_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elmo_in_smalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elmo_in_smalls[129])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### try this right after imports to load elmo_in_smalls\n",
    "elmo_in_smalls = np.load('/home/ec2-user/NLP_projects/job_recommender_project/elmo_resumes_under_100_sentences.npy', allow_pickle=True).tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many attempts\n",
    "============"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#NLP\n",
    "#import spacy\n",
    "import re\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# file loading\n",
    "import pickle\n",
    "\n",
    "#deep learning\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from time import process_time\n",
    "\n",
    "### try this right after imports to load elmo_in_smalls\n",
    "elmo_in_smalls = np.load('/home/ec2-user/NLP_projects/job_recommender_project/elmo_resumes_under_100_sentences.npy', allow_pickle=True).tolist()\n",
    "\n",
    "\n",
    "## this was the instantiation of elmo\n",
    "elmo = hub.Module(\"/home/ec2-user/module/module_elmo2\", trainable=False)\n",
    "\n",
    "# for all embeddings\n",
    "resume_embeddings_append_regular = []\n",
    "resume_embeddings_append_transpose = []\n",
    "resume_embeddings_extend_regular = []\n",
    "resume_embeddings_extend_transpose = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, 49)):#len(elmo_in_smalls))):\n",
    "    t1 = process_time()\n",
    "    # save np array of embeddings\n",
    "    if i % 50 == 0:\n",
    "        np_ELMo_embeddings_resumes = np.asarray(resume_embeddings_append_transpose)\n",
    "        np.save('ELMo_embeddings_resumes_401_to_{}'.format(i), np_ELMo_embeddings_resumes) \n",
    "    \n",
    "    print('elmo_in_smalls[{}]'.format(i))\n",
    "    embeddings_3d = elmo(elmo_in_smalls[i], signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "    print('3d size: {}'.format(embeddings_3d.shape))\n",
    "    print('3d type ', type(embeddings_3d))\n",
    "    t2 = process_time()\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        embeddings_2d = session.run(tf.reduce_mean(embeddings_3d,axis=0))\n",
    "        print('2d size: {}'.format(embeddings_2d.shape))\n",
    "        print('2d type ', type(embeddings_2d))\n",
    "        t3 = process_time()\n",
    "        \n",
    "        \n",
    "        embeddings_1d = session.run(tf.reduce_mean(tf.convert_to_tensor(embeddings_2d), axis = 0, keepdims=True))\n",
    "        print('1d size: {}'.format(embeddings_1d.shape))\n",
    "        print('1d type ', type(embeddings_1d))\n",
    "        t4 = process_time()\n",
    "        \n",
    "        transpose = embeddings_1d.T\n",
    "        print(transpose.shape)\n",
    "        print('transpose type: ', type(transpose))\n",
    "        t5 = process_time()\n",
    "        \n",
    "        resume_embeddings_append_regular.append(embeddings_1d)\n",
    "        t6 = process_time()\n",
    "        \n",
    "        resume_embeddings_append_transpose.append(transpose)\n",
    "        t7 = process_time()\n",
    "        \n",
    "        resume_embeddings_extend_regular.extend(embeddings_1d)\n",
    "        t8 = process_time()\n",
    "        \n",
    "        resume_embeddings_extend_transpose.extend(transpose)\n",
    "        t9 = process_time()\n",
    "        \n",
    "        print('length of resume_embeddings_append_regular: {}'.format(len(resume_embeddings_append_regular)))\n",
    "        print('length of resume_embeddings_append_transpose: {}'.format(len(resume_embeddings_append_transpose)))\n",
    "        print('length of resume_embeddings_extend_regular: {}'.format(len(resume_embeddings_extend_regular)))\n",
    "        print('length of resume_embeddings_extend_transpose: {}'.format(len(resume_embeddings_extend_transpose)))      \n",
    "        \n",
    "        print('shape of resume_embeddings_append_regular[{}]: {}'.format(i, resume_embeddings_append_regular[i].shape))\n",
    "        print('shape of resume_embeddings_append_transpose[{}]: {}'.format(i, resume_embeddings_append_transpose[i].shape))                                                                        \n",
    "        print('shape of resume_embeddings_extend_regular[{}]: {}'.format(i, resume_embeddings_extend_regular[i].shape))                                                                        \n",
    "        print('shape of resume_embeddings_extend_transpose[{}]: {}'.format(i, resume_embeddings_extend_transpose[i].shape))                                                                        \n",
    "        \n",
    "        #not .extend?\n",
    "        \n",
    "    \n",
    "    \n",
    "        print('Run {}'.format(i))\n",
    "        print('all times from start of run {}'.format(i))\n",
    "        print('3d embedding time: {}'.format(t2 - t1))\n",
    "        print('2d embedding time: {}'.format(t3 - t2))\n",
    "        print('1d embedding time: {}'.format(t4 - t3))\n",
    "        print('time to transpose (t5-t4): {}'.format(t5 - t4))\n",
    "        print('append_regular time (t6-t5): {}'.format(t6-t5))\n",
    "        print('append_transpose time (t7-t6): {}'.format(t7-t6))\n",
    "        print('extend_regular time (t8-t7): {}'.format(t8-t7))\n",
    "        print('extend_transpose time (t9-t8): {}'.format(t9-t8))\n",
    "        print('Total time run {}: {}'.format(i, t9-t1))         \n",
    "        print('\\n\\n\\n\\t\\t\\tEND OF RUN')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br><br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "======================================================\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving tf.sess out of for loop\n",
    "======================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#NLP\n",
    "#import spacy\n",
    "import re\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# file loading\n",
    "import pickle\n",
    "\n",
    "#deep learning\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from time import process_time\n",
    "import resource\n",
    "\n",
    "### try this right after imports to load elmo_in_smalls\n",
    "elmo_in_smalls = np.load('/home/ec2-user/NLP_projects/job_recommender_project/elmo_resumes_under_100_sentences.npy', allow_pickle=True).tolist()\n",
    "\n",
    "\n",
    "##################reset default graph tf###################\n",
    "tf.reset_default_graph()\n",
    "############################################\n",
    "\n",
    "## this was the instantiation of elmo\n",
    "elmo = hub.Module(\"/home/ec2-user/module/module_elmo2\", trainable=False)\n",
    "\n",
    "# for all embeddings\n",
    "resume_embeddings_append_regular = []\n",
    "resume_embeddings_append_transpose = []\n",
    "resume_embeddings_extend_regular = []\n",
    "resume_embeddings_extend_transpose = []\n",
    "\n",
    "\n",
    "print('\\n\\nstarting session\\n\\n')\n",
    "t0 = process_time()\n",
    "with tf.Session() as session:\n",
    "    t00 = process_time()\n",
    "    print('\\nstarting tf.Session took: {}'.format(t00-t0))\n",
    "    \n",
    "    t_pre_initialize_vars = process_time()\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    t_post_initialize_vars = process_time()\n",
    "    print('Time to initialize variables: {}'.format(t_post_initialize_vars-t_pre_initialize_vars))\n",
    "\n",
    "    \n",
    "    \n",
    "    print('\\n\\nstarting for loop\\n\\n')\n",
    "    for i in tqdm(range(0, 49)):#len(elmo_in_smalls))):\n",
    "        t1 = process_time()\n",
    "        # save np array of embeddings\n",
    "        if i % 50 == 0:\n",
    "            np_ELMo_embeddings_resumes = np.asarray(resume_embeddings_append_transpose)\n",
    "            np.save('new_ELMo_embeddings_resumes_0_to_{}'.format(i), np_ELMo_embeddings_resumes) \n",
    "\n",
    "        print('elmo_in_smalls[{}]'.format(i))\n",
    "        embeddings_3d = elmo(elmo_in_smalls[i], signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "        embeddings_3d_f16 = tf.dtypes.cast(embeddings_3d, tf.float16)\n",
    "        print('3d size: {}'.format(embeddings_3d_f16.shape))\n",
    "        print('3d type ', type(embeddings_3d_f16))\n",
    "        t2 = process_time()\n",
    "\n",
    "        \n",
    "        embeddings_2d = session.run(tf.reduce_mean(embeddings_3d_f16,axis=0))\n",
    "        print('2d size: {}'.format(embeddings_2d.shape))\n",
    "        print('2d type ', type(embeddings_2d))\n",
    "        t3 = process_time()\n",
    "\n",
    "\n",
    "        embeddings_1d = session.run(tf.reduce_mean(tf.convert_to_tensor(embeddings_2d), axis = 0, keepdims=True))\n",
    "        print('1d size: {}'.format(embeddings_1d.shape))\n",
    "        print('1d type ', type(embeddings_1d))\n",
    "        t4 = process_time()\n",
    "\n",
    "        transpose = embeddings_1d.T\n",
    "        print(transpose.shape)\n",
    "        print('transpose type: ', type(transpose))\n",
    "        t5 = process_time()\n",
    "\n",
    "        resume_embeddings_append_regular.append(embeddings_1d)\n",
    "        t6 = process_time()\n",
    "\n",
    "        resume_embeddings_append_transpose.append(transpose)\n",
    "        t7 = process_time()\n",
    "\n",
    "        resume_embeddings_extend_regular.extend(embeddings_1d)\n",
    "        t8 = process_time()\n",
    "\n",
    "        resume_embeddings_extend_transpose.extend(transpose)\n",
    "        t9 = process_time()\n",
    "\n",
    "        print('length of resume_embeddings_append_regular: {}'.format(len(resume_embeddings_append_regular)))\n",
    "        print('length of resume_embeddings_append_transpose: {}'.format(len(resume_embeddings_append_transpose)))\n",
    "        print('length of resume_embeddings_extend_regular: {}'.format(len(resume_embeddings_extend_regular)))\n",
    "        print('length of resume_embeddings_extend_transpose: {}'.format(len(resume_embeddings_extend_transpose)))      \n",
    "\n",
    "        print('shape of resume_embeddings_append_regular[{}]: {}'.format(i, resume_embeddings_append_regular[i].shape))\n",
    "        print('shape of resume_embeddings_append_transpose[{}]: {}'.format(i, resume_embeddings_append_transpose[i].shape))                                                                        \n",
    "        print('shape of resume_embeddings_extend_regular[{}]: {}'.format(i, resume_embeddings_extend_regular[i].shape))                                                                        \n",
    "        print('shape of resume_embeddings_extend_transpose[{}]: {}'.format(i, resume_embeddings_extend_transpose[i].shape))                                                                        \n",
    "\n",
    "        #not .extend?\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nRun {}'.format(i))\n",
    "        \n",
    "        print('Iteration ', i, ' maxrss: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "        print('graph size: {}'.format(tf.graph.shape))\n",
    "        print('\\nall times from start of run {}\\n'.format(i))\n",
    "        print('3d embedding time: {}'.format(t2 - t1))\n",
    "        print('2d embedding time: {}'.format(t3 - t2))\n",
    "        print('1d embedding time: {}'.format(t4 - t3))\n",
    "        print('time to transpose (t5-t4): {}'.format(t5 - t4))\n",
    "        print('append_regular time (t6-t5): {}'.format(t6-t5))\n",
    "        print('append_transpose time (t7-t6): {}'.format(t7-t6))\n",
    "        print('extend_regular time (t8-t7): {}'.format(t8-t7))\n",
    "        print('extend_transpose time (t9-t8): {}'.format(t9-t8))\n",
    "        print('Total time run {}: {}'.format(i, t9-t1))         \n",
    "        print('\\n\\n\\n\\t\\t\\tEND OF RUN')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding graph problem?? *******TRY ME********\n",
    "=====================\n",
    "\n",
    "\n",
    "- maybe the answer is to elmo_3d every resume...then do something?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### data science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#NLP\n",
    "#import spacy\n",
    "import re\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# file loading\n",
    "import pickle\n",
    "\n",
    "#deep learning\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from time import process_time\n",
    "import resource\n",
    "\n",
    "### try this right after imports to load elmo_in_smalls\n",
    "elmo_in_smalls = np.load('/home/ec2-user/NLP_projects/job_recommender_project/elmo_resumes_under_100_sentences.npy', allow_pickle=True).tolist()\n",
    "\n",
    "\n",
    "##################reset default graph tf###################\n",
    "tf.reset_default_graph()\n",
    "############################################\n",
    "\n",
    "## this was the instantiation of elmo\n",
    "elmo = hub.Module(\"/home/ec2-user/module/module_elmo2\", trainable=False)\n",
    "\n",
    "# for all embeddings\n",
    "resume_embeddings_append_regular = []\n",
    "resume_embeddings_append_transpose = []\n",
    "resume_embeddings_extend_regular = []\n",
    "resume_embeddings_extend_transpose = []\n",
    "\n",
    "\n",
    "print('\\n\\nstarting session\\n\\n')\n",
    "t0 = process_time()\n",
    "with tf.Session() as session:\n",
    "    t00 = process_time()\n",
    "    print('\\nstarting tf.Session took: {}'.format(t00-t0))\n",
    "    \n",
    "    t_pre_initialize_vars = process_time()\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    t_post_initialize_vars = process_time()\n",
    "    print('Time to initialize variables: {}'.format(t_post_initialize_vars-t_pre_initialize_vars))\n",
    "\n",
    "    \n",
    "    \n",
    "    print('\\n\\nstarting for loop\\n\\n')\n",
    "    for i in tqdm(range(0, 200)):#len(elmo_in_smalls))):\n",
    "        t1 = process_time()\n",
    "        # save np array of embeddings\n",
    "        if i % 50 == 0:\n",
    "            np_ELMo_embeddings_resumes = np.asarray(resume_embeddings_append_transpose)\n",
    "            np.save('new_ELMo_embeddings_resumes_0_to_{}'.format(i), np_ELMo_embeddings_resumes) \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "#         assign_op = x.assign(1)\n",
    "#         sess.run(assign_op)  # or `assign_op.op.run()`\n",
    "#         print(x.eval())\n",
    "    \n",
    "        print('elmo_in_smalls[{}]'.format(i))\n",
    "        embeddings_3d = elmo(elmo_in_smalls[i], signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "        print('3d type original', type(embeddings_3d))\n",
    "        t_make_np_start = process_time()\n",
    "        embeddings_3d_np = embeddings_3d.eval()\n",
    "        t_make_np_stop = process_time()          \n",
    "        print('3d type np?', type(embeddings_3d_np))\n",
    "        \n",
    "        \n",
    "        \n",
    "        embeddings_3d_np_f16 = embeddings_3d_np #tf.dtypes.cast(embeddings_3d_np, tf.float16)\n",
    "        print('3d size: {}'.format(embeddings_3d_np_f16.shape))\n",
    "        print('3d type np? 16?', type(embeddings_3d_np_f16))\n",
    "        t2 = process_time()\n",
    "\n",
    "        \n",
    "        embeddings_2d = np.mean(embeddings_3d_np_f16,axis=0)#session.run(tf.reduce_mean(embeddings_3d_np_f16,axis=0))\n",
    "        print('2d size: {}'.format(embeddings_2d.shape))\n",
    "        print('2d type ', type(embeddings_2d))\n",
    "        t3 = process_time()\n",
    "\n",
    "\n",
    "        embeddings_1d = np.mean(embeddings_2d,axis=0)#session.run(tf.reduce_mean(tf.convert_to_tensor(embeddings_2d), axis = 0, keepdims=True))\n",
    "        print('1d size: {}'.format(embeddings_1d.shape))\n",
    "        print('1d type ', type(embeddings_1d))\n",
    "        t4 = process_time()\n",
    "\n",
    "        transpose = embeddings_1d.T\n",
    "        print(transpose.shape)\n",
    "        print('transpose type: ', type(transpose))\n",
    "        t5 = process_time()\n",
    "\n",
    "        resume_embeddings_append_regular.append(embeddings_1d)\n",
    "        t6 = process_time()\n",
    "\n",
    "        resume_embeddings_append_transpose.append(transpose)\n",
    "        t7 = process_time()\n",
    "\n",
    "        resume_embeddings_extend_regular.extend(embeddings_1d)\n",
    "        t8 = process_time()\n",
    "\n",
    "        resume_embeddings_extend_transpose.extend(transpose)\n",
    "        t9 = process_time()\n",
    "\n",
    "        print('length of resume_embeddings_append_regular: {}'.format(len(resume_embeddings_append_regular)))\n",
    "        print('length of resume_embeddings_append_transpose: {}'.format(len(resume_embeddings_append_transpose)))\n",
    "        print('length of resume_embeddings_extend_regular: {}'.format(len(resume_embeddings_extend_regular)))\n",
    "        print('length of resume_embeddings_extend_transpose: {}'.format(len(resume_embeddings_extend_transpose)))      \n",
    "\n",
    "        print('shape of resume_embeddings_append_regular[{}]: {}'.format(i, resume_embeddings_append_regular[i].shape))\n",
    "        print('shape of resume_embeddings_append_transpose[{}]: {}'.format(i, resume_embeddings_append_transpose[i].shape))                                                                        \n",
    "        print('shape of resume_embeddings_extend_regular[{}]: {}'.format(i, resume_embeddings_extend_regular[i].shape))                                                                        \n",
    "        print('shape of resume_embeddings_extend_transpose[{}]: {}'.format(i, resume_embeddings_extend_transpose[i].shape))                                                                        \n",
    "\n",
    "        #not .extend?\n",
    "\n",
    "\n",
    "\n",
    "        print('\\nRun {}'.format(i))\n",
    "        \n",
    "        print('Iteration ', i, ' maxrss: ', resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)\n",
    "        \n",
    "        print('\\nall times from start of run {}\\n'.format(i))\n",
    "        print('3d embedding time: {}'.format(t_make_np_start - t1))\n",
    "        print('3d tensor to np time: {}'.format(t_make_np_stop - t_make_np_start))\n",
    "        print('3d make 16b time: {}'.format(t2-t_make_np_stop))\n",
    "        print('2d embedding time: {}'.format(t3 - t2))\n",
    "        print('1d embedding time: {}'.format(t4 - t3))\n",
    "        print('time to transpose (t5-t4): {}'.format(t5 - t4))\n",
    "        print('append_regular time (t6-t5): {}'.format(t6-t5))\n",
    "        print('append_transpose time (t7-t6): {}'.format(t7-t6))\n",
    "        print('extend_regular time (t8-t7): {}'.format(t8-t7))\n",
    "        print('extend_transpose time (t9-t8): {}'.format(t9-t8))\n",
    "        print('Total time run {}: {}'.format(i, t9-t1))         \n",
    "        print('\\n\\n\\n\\t\\t\\tEND OF RUN')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_embeddings_append_transpose[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this was the instantiation of elmo\n",
    "elmo = hub.Module(\"/home/ec2-user/module/module_elmo2\", trainable=False)\n",
    "\n",
    "\n",
    "# for all embeddings\n",
    "\n",
    "resume_embeddings = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, 1)):#len(elmo_in_smalls))):\n",
    "    # save np array of embeddings\n",
    "    if i % 50 == 0:\n",
    "        np_ELMo_embeddings_resumes = np.asarray(resume_embeddings)\n",
    "        np.save('ELMo_embeddings_resumes_401_to_{}'.format(i), np_ELMo_embeddings_resumes) \n",
    "    \n",
    "    print('elmo_in_smalls[{}]'.format(i))\n",
    "    embeddings_3d = elmo(elmo_in_smalls[i], signature=\"default\",as_dict=True)[\"elmo\"]\n",
    "    print('3d size: {}'.format(embeddings_3d.shape))\n",
    "    print('3d type ', type(embeddings_3d))\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        embeddings_2d = session.run(tf.reduce_mean(embeddings_3d,axis=0))\n",
    "        print('2d size: {}'.format(embeddings_2d.shape))\n",
    "        print('2d type ', type(embeddings_2d))\n",
    "        \n",
    "        embeddings_1d = session.run(tf.reduce_mean(tf.convert_to_tensor(embeddings_2d), axis = 0, keepdims=True))\n",
    "        print('1d size: {}'.format(embeddings_1d.shape))\n",
    "        print('1d type ', type(embeddings_1d))\n",
    "        \n",
    "        transpose = embeddings_1d.T\n",
    "        print(transpose.shape)\n",
    "        print('transpose type: ', type(transpose))\n",
    "\n",
    "        resume_embeddings.append(transpose)\n",
    "        \n",
    "        print('length of resume_embeddings: {}'.format(len(resume_embeddings)))\n",
    "        print('shape of resume_embeddings[{}]: {}'.format(i, resume_embeddings[i].shape))\n",
    "        #not .extend?\n",
    "\n",
    "        \n",
    "        #####closing session at everytime\n",
    "        session.close()\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resume_embeddings_append_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_embeddings_append_regular[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_embeddings_append_transpose[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f32 = 3/1048\n",
    "f32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f16 = np.float16(3/1048)\n",
    "f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(resume_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(resume_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(resume_embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save np array of embeddings\n",
    "# np_ELMo_embeddings_resumes = np.asarray(resume_embeddings)\n",
    "# np.save('ELMo_embeddings_resumes_401_{}'.format(len(elmo_in_smalls)', np_ELMo_embeddings_resumes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save elmo_train_new\n",
    "# pickle_out = open(\"ELMo_embeddings_resumes.pickle\",\"wb\")\n",
    "# pickle.dump(np_ELMo_embeddings_resumes, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
